{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G7VlWlunGMj",
        "outputId": "1901f4e1-afc2-4fdf-a886-2afc94480c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.83)] [\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.83)] [\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.83)] [\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tor is already the newest version (0.4.6.10-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            " * Starting tor daemon...\n",
            "   ...done.\n"
          ]
        }
      ],
      "source": [
        "# Install Tor\n",
        "!apt-get update\n",
        "!apt-get install tor -y\n",
        "\n",
        "# Start the Tor service\n",
        "!service tor start\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now run your original script\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import csv\n",
        "import re # Import the re module\n",
        "\n",
        "# Proxy para Tor\n",
        "proxies = {\n",
        "    'http': 'socks5h://localhost:9050',\n",
        "    'https': 'socks5h://localhost:9050'\n",
        "}\n",
        "\n",
        "ahmia_url = 'https://ahmia.fi/onions'\n",
        "\n",
        "try:\n",
        "    # Hacemos la petición a través de Tor\n",
        "    response = requests.get(ahmia_url, proxies=proxies, timeout=60) # Increased timeout\n",
        "    response.raise_for_status() # Levanta una excepción para códigos de estado de error HTTP\n",
        "\n",
        "    # Move the code that uses 'response' inside the try block\n",
        "    # No need for BeautifulSoup here if we're extracting directly from text\n",
        "    # soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # *** Add this line to inspect the HTML content ***\n",
        "    print(\"\\n--- HTML Content Sample ---\")\n",
        "    print(response.text[:1000]) # Print first 1000 characters\n",
        "    print(\"---------------------------\\n\")\n",
        "\n",
        "    # Extraemos todos los enlaces .onion usando regex on the full text\n",
        "    onions = set()\n",
        "    # Regex to find v3 onion addresses, optionally followed by a path\n",
        "    onion_pattern = re.compile(r'\\b(?:https?://)?[a-z0-9]{16,56}\\.onion(?:\\/[^\\s<]*)?\\b')\n",
        "\n",
        "    # Find all matches in the response text\n",
        "    found_onions = onion_pattern.findall(response.text)\n",
        "\n",
        "    for onion_url in found_onions:\n",
        "        # Basic cleaning: remove trailing slashes if present and ensure http:// prefix (optional depending on desired format)\n",
        "        clean_url = onion_url.strip()\n",
        "        if not clean_url.startswith('http'):\n",
        "             clean_url = 'http://' + clean_url # Add http:// if missing\n",
        "        # Remove trailing slash if it's just the root directory\n",
        "        if clean_url.endswith('/') and len(clean_url) > len('http://') + 56 + 7: # 56 for hash, 7 for .onion/\n",
        "            clean_url = clean_url.rstrip('/')\n",
        "\n",
        "        onions.add(clean_url)\n",
        "\n",
        "\n",
        "    print(f\"Encontradas {len(onions)} direcciones .onion\")\n",
        "\n",
        "    # Guardar en TXT\n",
        "    with open('onions_list.txt', 'w') as f:\n",
        "        for onion in sorted(onions):\n",
        "            f.write(onion + '\\n')\n",
        "\n",
        "    # Guardar en JSON\n",
        "    with open('onions_list.json', 'w', encoding='utf-8') as f:\n",
        "        # json.dump takes a list, set needs to be converted\n",
        "        json.dump(sorted(list(onions)), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Guardar en CSV\n",
        "    with open('onions_list.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['onion_url'])\n",
        "        for onion in sorted(onions):\n",
        "            writer.writerow([onion])\n",
        "\n",
        "    print(\"Archivos generados: onions_list.txt, .json y .csv\")\n",
        "\n",
        "\n",
        "except requests.exceptions.ConnectionError as e:\n",
        "    print(f\"Error al conectar con el proxy o el servidor: {e}\")\n",
        "    print(\"Por favor, asegúrate de que el servicio Tor esté funcionando y sea accesible en localhost:9050.\")\n",
        "    # Keep exit() here as connection error means we cannot proceed\n",
        "    exit()\n",
        "except requests.exceptions.Timeout as e:\n",
        "    print(f\"Tiempo de espera agotado al conectar a {ahmia_url} a través del proxy: {e}\")\n",
        "    # Keep exit() here as timeout means we cannot get the data\n",
        "    exit()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Ocurrió un error durante la petición: {e}\")\n",
        "    # Keep exit() here for other request errors\n",
        "    exit()\n",
        "# Consider adding a general Exception catch for unexpected errors\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error inesperado: {e}\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XViLudsWqh_k",
        "outputId": "3a152ea0-5f5f-4b3c-bd1d-760355866cfb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- HTML Content Sample ---\n",
            " \n",
            "  http://222222223bmct6m464moskwt5hxgz2hj2wbsh224oh4m3rfe6e7olhqd.onion/<br>\n",
            "  \n",
            "  http://222222227nt67h4vw6y3a547k24kal6en5kvd3janyqmi4wmrvq4afid.onion/<br>\n",
            "  \n",
            "  http://22222222jwij2awdpkcpyxv3mfx4aeqzvpl52t7qppgimf4ls7mv3jqd.onion/<br>\n",
            "  \n",
            "  http://22222222yttarxo4r6kxpqfduphh2bjy24xflod4ikft7l62dcxm5nqd.onion/<br>\n",
            "  \n",
            "  http://22222222zkmeso7rx2dd3f3kykd7icqgiuf4uy4bar5dl74m5llew2yd.onion/<br>\n",
            "  \n",
            "  http://22222224i4yvqgfah2iaxb4kgqlqsaz26tlibhngc772eafzbvuapyyd.onion/<br>\n",
            "  \n",
            "  http://22222224qvkicqoe5fbaclkzsbf6i7ktp57oue2oemkrcqdfxxgp3bid.onion/<br>\n",
            "  \n",
            "  http://22222225hxbth2cckelbt52vtbfvkuw67odl77kaggyekcgwlou3lfad.onion/<br>\n",
            "  \n",
            "  http://22222225kmpwlvfhwt56xdlj2yerqu64fwgrnqh6giam33f2zx3bm3id.onion/<br>\n",
            "  \n",
            "  http://222222262jv7a7hazfgre7pfrb2s5xrgcjsdzcv53jndef7yjvqqlcqd.onion/<br>\n",
            "  \n",
            "  http://22222226nbvy3b4w73gg5h3g2dnlltshvua3cskxe2ynzvgcplx725ad.onion/<br>\n",
            "  \n",
            "  http://2222222757uebc4nj4aqtrjxsr3ywf7w6lcks47uvxqdjk5b5h3m7bid.onion/<br>\n",
            "  \n",
            "  http://2222222b3w4vhmvmcbpucodccf6tq\n",
            "---------------------------\n",
            "\n",
            "Encontradas 16324 direcciones .onion\n",
            "Archivos generados: onions_list.txt, .json y .csv\n"
          ]
        }
      ]
    }
  ]
}